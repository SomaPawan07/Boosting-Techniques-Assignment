{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. What is Boosting in Machine Learning? Explain how it improves weak\n",
        "learners.**\n",
        "- Boosting is an ensemble technique that combines multiple weak learners (usually shallow decision trees) to create a strong, accurate model.\n",
        "\n",
        "How Boosting Improves Weak Learners:\n",
        "- Initialize the model by training a weak learner on the full dataset.\n",
        "- Evaluate errors made by this learner.\n",
        "- Assign higher weights to the misclassified samples (or larger residuals in regression).\n",
        "- Train the next model with increased focus on these \"hard\" cases.\n",
        "- Repeat the process, combining all models into a weighted sum (final model).\n",
        "- Final prediction is made using a weighted vote or sum of all the weak learners."
      ],
      "metadata": {
        "id": "YhdGK_5tfYcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2. What is the difference between AdaBoost and Gradient Boosting in terms of how models are trained?**\n",
        "- | Feature                | **AdaBoost**                                                                                               | **Gradient Boosting**                                                       |\n",
        "| ---------------------- | ---------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------- |\n",
        "| **Core Idea**          | Focuses on **reweighting** misclassified samples                                                           | Fits the model to **residual errors** (gradients of a loss function)        |\n",
        "| **Error Handling**     | Increases weights on **incorrectly predicted samples** so the next learner focuses on them                 | Models the **gradient (error)** of the loss function to minimize it         |\n",
        "| **Weighting**          | Each weak learner is assigned a **weight** based on accuracy; samples get **new weights** after each round | Learners are **added to minimize loss**, no explicit reweighting of samples |\n",
        "| **Loss Function**      | Uses **exponential loss** by default                                                                       | Can use **custom differentiable loss functions** (e.g., MSE, log loss)      |\n",
        "| **Output Aggregation** | Final prediction = **weighted sum** of weak learners                                                       | Final prediction = **sum of predictions** from all learners                 |\n",
        "| **Interpretation**     | More intuitive (focus on misclassified examples)                                                           | More flexible and powerful (gradient-based optimization)                    |\n"
      ],
      "metadata": {
        "id": "QCNHFUrNg4Zw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3. How does regularization help in XGBoost?**\n",
        "- XGBoost (Extreme Gradient Boosting) is a powerful and efficient implementation of gradient boosting ‚Äî and one of the reasons it performs so well is its built-in regularization mechanisms.\n",
        "\n",
        "How Regularization Works in XGBoost\n",
        "\n",
        "In XGBoost, regularization is directly integrated into the objective function:\n",
        "\n",
        "Objective\n",
        "=\n",
        "Loss\n",
        "(\n",
        "predictions\n",
        ",\n",
        "actuals\n",
        ")\n",
        "+\n",
        "Œ©\n",
        "(\n",
        "ùëì\n",
        ")\n",
        "Objective=Loss(predictions,actuals)+Œ©(f)\n",
        "\n",
        "Where:\n",
        "\n",
        "- Loss: Measures model's prediction error (e.g., MSE, log loss)\n",
        "\n",
        "- Œ©(f): Regularization term to penalize complex trees"
      ],
      "metadata": {
        "id": "zBjhLFe8iMhm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4. Why is CatBoost considered efficient for handling categorical data?**\n",
        "- Key Reasons Why CatBoost Is Efficient for Categorical Features:\n",
        "1. No Need for Manual Encoding\n",
        "- We don‚Äôt need to apply one-hot encoding or label encoding manually.\n",
        "- Just pass categorical column indices or names ‚Äî CatBoost does the rest.\n",
        "\n",
        "- Saves time, reduces feature engineering effort, and avoids dimensionality explosion caused by one-hot encoding.\n",
        "\n",
        "2. Uses Target-Based Statistics with Built-in Regularization\n",
        "CatBoost converts categorical features using target statistics like:\n",
        "\n",
        "                       Value=count¬†of¬†category‚àëi‚ààcategory‚Äãtargeti‚Äã‚Äã\n",
        "But with a twist:\n",
        "- It uses ordered boosting (see below) to avoid target leakage\n",
        "- It adds noise/randomization to the encoding for regularization\n",
        "- This allows the model to extract predictive power from categories without overfitting\n",
        "\n",
        "3. Ordered Boosting Prevents Target Leakage\n",
        "- Traditional target encoding can accidentally \"peek\" at the true target values of a sample while computing statistics ‚Äî this leads to overfitting.\n",
        "- CatBoost uses ordered boosting, where:\n",
        "- It calculates statistics for a data point using only previous data points (based on a random permutation)\n",
        "- Ensures no information leak from the target\n",
        "- Improves generalization and makes model training more stable.\n",
        "  \n",
        "4. Optimized for Speed and Accuracy\n",
        "- Categorical handling is done internally and efficiently, avoiding the overhead of creating many binary columns\n",
        "- CatBoost supports GPU training, early stopping, and built-in hyperparameter tuning\n",
        "- Works well out-of-the-box, with fewer tweaks needed compared to other libraries"
      ],
      "metadata": {
        "id": "Lt9kBU6yjuYQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5. What are some real-world applications where boosting techniques are\n",
        "preferred over bagging methods?**\n",
        "- Real-World Applications Where Boosting is Preferred\n",
        "1.Credit Scoring & Loan Default Prediction\n",
        "Why Boosting?\n",
        "- Boosting algorithms (e.g. XGBoost, LightGBM) handle imbalanced classes well.\n",
        "- Can focus on rare but important events like defaults.\n",
        "- Goal: Accurately predict default risk without high false negatives.\n",
        "\n",
        "2. Fraud Detection\n",
        "Why Boosting?\n",
        "- Fraud cases are rare, so class imbalance is severe.\n",
        "- Boosting learns from misclassifications and improves detection of rare frauds.\n",
        "- Goal: Detect fraudulent transactions with minimal false positives/negatives.\n",
        "\n",
        "3. Medical Diagnosis / Risk Prediction\n",
        "Why Boosting?\n",
        "- Captures complex feature interactions better than bagging.\n",
        "- Provides high accuracy needed for life-critical predictions.\n",
        "- Goal: Predict disease risk, survival rates, or treatment outcomes with precision.\n",
        "4. Customer Churn Prediction / Marketing Response\n",
        "Why Boosting?\n",
        "- Models subtle patterns in customer behavior.\n",
        "\n",
        "- oosting can improve precision and recall in predicting churners or responders.\n",
        "- Goal: Optimize marketing spend or retention strategies.\n",
        "\n",
        "5. Search Ranking / Recommendation Systems\n",
        "- Why Boosting?\n",
        "- Frameworks like LambdaMART (a boosted tree version) are used for ranking tasks.\n",
        "- Excellent for optimizing metrics like NDCG or click-through rate.\n",
        "\n",
        "- Goal: Rank content, ads, or products effectively."
      ],
      "metadata": {
        "id": "E0F0B39IkvJ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6. Datasets:**\n",
        "\n",
        "‚óè**Use sklearn.datasets.load_breast_cancer() for classification tasks.**\n",
        "\n",
        "‚óè **Use sklearn.datasets.fetch_california_housing() for regression\n",
        "tasks.**\n",
        "\n",
        "**Write a Python program to:**\n",
        "\n",
        "‚óè **Train an AdaBoost Classifier on the Breast Cancer dataset**\n",
        "\n",
        "‚óè **Print the model accuracy**"
      ],
      "metadata": {
        "id": "SF7rD4EElohF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "model = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"AdaBoost Classifier Accuracy on Breast Cancer Dataset:\", round(accuracy, 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9ffX2YAmI5V",
        "outputId": "10b60af7-7588-46c9-94b9-9ff2ddb30b8c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Classifier Accuracy on Breast Cancer Dataset: 0.9708\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7. Write a Python program to:**\n",
        "\n",
        "‚óè **Train a Gradient Boosting Regressor on the California Housing dataset**\n",
        "\n",
        "‚óè **Evaluate performance using R-squared score**"
      ],
      "metadata": {
        "id": "QFlABgMTmAXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Gradient Boosting Regressor R¬≤ Score on California Housing Dataset:\", round(r2, 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrNsHqclmujb",
        "outputId": "3052c3dc-c597-49b4-ea1d-4f4343bc42f3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Regressor R¬≤ Score on California Housing Dataset: 0.7803\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q8.  Write a Python program to:**\n",
        "\n",
        "‚óè **Train an XGBoost Classifier on the Breast Cancer dataset**\n",
        "\n",
        "‚óè **Tune the learning rate using GridSearchCV**\n",
        "\n",
        "‚óè **Print the best parameters and accuracy**"
      ],
      "metadata": {
        "id": "CiMJwhoymcBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=model,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Test Set Accuracy:\", round(accuracy, 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3d5MfJunOf-",
        "outputId": "736fbd08-8e8c-470e-b6a6-65b6e9a1345e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'learning_rate': 0.3}\n",
            "Test Set Accuracy: 0.9649\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [16:58:32] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q9. Write a Python program to:**\n",
        "\n",
        "‚óè **Train a CatBoost Classifier**\n",
        "\n",
        "‚óè **Plot the confusion matrix using seaborn**"
      ],
      "metadata": {
        "id": "vGAf7J9Km7ew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "model = CatBoostClassifier(verbose=0, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "labels = data.target_names  # ['malignant', 'benign']\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
        "plt.title(\"Confusion Matrix - CatBoost Classifier\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "YnFkEdcSnx0m",
        "outputId": "5f9f958a-6f74-41ca-a685-0ee7ab8a6a4b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'catboost'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1962556324.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcatboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCatBoostClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_breast_cancer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConfusionMatrixDisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'catboost'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q10. You're working for a FinTech company trying to predict loan default using customer demographics and transaction behavior.\n",
        "The dataset is imbalanced, contains missing values, and has both numeric and categorical features.\n",
        "Describe your step-by-step data science pipeline using boosting techniques:\n",
        "‚óè Data preprocessing & handling missing/categorical values\n",
        "‚óè Choice between AdaBoost, XGBoost, or CatBoost\n",
        "‚óè Hyperparameter tuning strategy\n",
        "‚óè Evaluation metrics you'd choose and why\n",
        "‚óè How the business**\n",
        "\n",
        "- Step-by-Step Data Science Pipeline\n",
        "1. Data Preprocessing\n",
        "a. Handle Missing Values\n",
        "- Numeric features:\n",
        "- Use median imputation (robust to outliers)\n",
        "- Optionally, add a missingness indicator feature\n",
        "- Categorical features:\n",
        "- Use \"Missing\" or \"Unknown\" category\n",
        "- Or use built-in handling (e.g., CatBoost natively supports missing value\n",
        "\n",
        "2. Choice of Boosting Algorithm\n",
        "\n",
        "| Algorithm      | When to Use                                                                                              |\n",
        "| -------------- | -------------------------------------------------------------------------------------------------------- |\n",
        "| **AdaBoost**   | Simple datasets, fewer categorical features, not optimal here                                            |\n",
        "| **XGBoost**    | Highly accurate, needs manual encoding for categoricals                                                  |\n",
        "| **CatBoost** ‚úÖ | **Best for this case**: handles categorical + missing data natively, works well with imbalanced datasets |\n",
        "\n",
        "3. Handling Imbalanced Data\n",
        "- Use class weights or scale_pos_weight (XGBoost), or class_weights='Balanced' (CatBoost)\n",
        "- SMOTE or resampling techniques (optional, use with caution for boosting)\n",
        "\n",
        "4. Hyperparameter Tuning Strategy\n",
        "a. Initial Parameters to Tune:"
      ],
      "metadata": {
        "id": "REdDcyTPnmc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'depth': [4, 6, 8],\n",
        "    'l2_leaf_reg': [1, 3, 5],\n",
        "    'iterations': [100, 300]\n",
        "}\n"
      ],
      "metadata": {
        "id": "9KQEryAdooJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b. Tuning Approach:\n",
        "- Use GridSearchCV or RandomizedSearchCV with StratifiedKFold\n",
        "- Consider CatBoost‚Äôs built-in CV (faster and GPU-compatible)\n",
        "- Apply early stopping with a validation set\n",
        "\n",
        "5. Evaluation Metrics\n",
        "- Since the dataset is imbalanced, accuracy is not enough.\n",
        "\n",
        "| Metric                   | Why it Matters                                           |\n",
        "| ------------------------ | -------------------------------------------------------- |\n",
        "| **AUC-ROC** ‚úÖ            | Measures class separability; robust to imbalance         |\n",
        "| **F1-Score** ‚úÖ           | Balances precision and recall                            |\n",
        "| **Recall (Sensitivity)** | Important to catch actual defaulters                     |\n",
        "| **Precision**            | Prevents false alarms (non-defaulters marked as default) |\n",
        "| **Confusion Matrix**     | Gives full picture of model behavior                     |\n",
        "\n",
        "6. How This Helps the Business\n",
        "\n",
        "| Business Goal                     | Model Benefit                                                                 |\n",
        "| --------------------------------- | ----------------------------------------------------------------------------- |\n",
        "| **Reduce financial losses**       | Accurately flag likely defaulters early                                       |\n",
        "| **Optimize lending decisions**    | Better risk assessment ‚Üí smarter approvals                                    |\n",
        "| **Improve customer segmentation** | Identify low-risk customers for better terms                                  |\n",
        "| **Compliance & fairness**         | Use interpretable models (CatBoost offers feature importance and SHAP values) |\n"
      ],
      "metadata": {
        "id": "27ItPohZoplZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gVXi_PItoJWr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a9NkzUFroI4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sv7TFE52fTBH"
      },
      "outputs": [],
      "source": []
    }
  ]
}